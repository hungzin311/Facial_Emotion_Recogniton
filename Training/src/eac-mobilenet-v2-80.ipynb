{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8250896,"sourceType":"datasetVersion","datasetId":4895652},{"sourceId":56317,"sourceType":"modelInstanceVersion","modelInstanceId":47307}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport csv\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport pickle\nimport wandb\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport torchvision.models as models\nimport torch.utils.data as data\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T13:23:11.099352Z","iopub.execute_input":"2024-06-08T13:23:11.099756Z","iopub.status.idle":"2024-06-08T13:23:11.106669Z","shell.execute_reply.started":"2024-06-08T13:23:11.099728Z","shell.execute_reply":"2024-06-08T13:23:11.105624Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Args:\n  def __init__(self):\n    self.raf_path = '/kaggle/input/raf-basic/raf-basic'\n    self.mobilenet_v2_path = '/kaggle/input/mobilenet_v2/pytorch/mobilenet_v2/1/mobilenet_v2-b0353104.pth'\n    self.label_path = 'list_patition_label.txt'\n    self.workers = 2\n    self.batch_size = 64\n    self.w = 7\n    self.h = 7\n    self.gpu = 0\n    self.lam = 5.0\n    self.epochs = 8\n\nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.108625Z","iopub.execute_input":"2024-06-08T13:23:11.109074Z","iopub.status.idle":"2024-06-08T13:23:11.124187Z","shell.execute_reply.started":"2024-06-08T13:23:11.109041Z","shell.execute_reply":"2024-06-08T13:23:11.123294Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:56.716449Z","iopub.execute_input":"2024-05-08T09:18:56.716877Z","iopub.status.idle":"2024-05-08T09:18:56.723519Z","shell.execute_reply.started":"2024-05-08T09:18:56.716844Z","shell.execute_reply":"2024-05-08T09:18:56.722306Z"}}},{"cell_type":"code","source":"\n\ndef _make_divisible(v, divisor, min_value=None):\n  \n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, norm_layer=None):\n        padding = (kernel_size - 1) // 2\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            norm_layer(out_planes),\n            nn.ReLU6(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio, norm_layer=None):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            norm_layer(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self,\n                 num_classes=1000,\n                 width_mult=1.0,\n                 inverted_residual_setting=None,\n                 round_nearest=8,\n                 block=None,\n                 norm_layer=None):\n        \n        super(MobileNetV2, self).__init__()\n\n        if block is None:\n            block = InvertedResidual\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2, norm_layer=norm_layer)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def _forward_impl(self, x):\n        # This exists since TorchScript doesn't support inheritance, so the superclass method\n        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n        x = self.features(x)\n        # Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]\n        x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.125520Z","iopub.execute_input":"2024-06-08T13:23:11.125812Z","iopub.status.idle":"2024-06-08T13:23:11.150648Z","shell.execute_reply.started":"2024-06-08T13:23:11.125790Z","shell.execute_reply":"2024-06-08T13:23:11.149824Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\n\nclass Model(nn.Module):\n    def __init__(self, num_classes =  7):\n        super(Model, self).__init__()\n        mobilenet_v2 = MobileNetV2()\n        mobilenet_v2.load_state_dict(torch.load(args.mobilenet_v2_path))\n        self.features = nn.Sequential(*list(mobilenet_v2.children())[:-1])\n        \n        self.drop = nn.Dropout(0.5)\n        self.fc = nn.Linear(1280, 7)\n\n    def forward(self, x):\n        x = self.features(x)\n\n        feature = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n\n        feature = feature.view(feature.size(0), -1)\n        output = self.fc(feature)\n        \n        params = list(self.parameters())\n        fc_weights = params[-2].data\n        fc_weights = fc_weights.view(1, 7, 1280, 1, 1)\n        fc_weights = Variable(fc_weights, requires_grad = False)\n\n        # attention\n        feat = x.unsqueeze(1) # N * 1 * C * H * W\n        hm = feat * fc_weights\n        hm = hm.sum(2) # N * self.num_labels * H * W\n\n        return output, hm\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.152548Z","iopub.execute_input":"2024-06-08T13:23:11.153336Z","iopub.status.idle":"2024-06-08T13:23:11.162301Z","shell.execute_reply.started":"2024-06-08T13:23:11.153312Z","shell.execute_reply":"2024-06-08T13:23:11.161528Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Model()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-08T13:23:11.163743Z","iopub.execute_input":"2024-06-08T13:23:11.164204Z","iopub.status.idle":"2024-06-08T13:23:11.299907Z","shell.execute_reply.started":"2024-06-08T13:23:11.164172Z","shell.execute_reply":"2024-06-08T13:23:11.298977Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"\ndef add_g(image_array, mean=0.0, var=30):\n    std = var ** 0.5\n    image_add = image_array + np.random.normal(mean, std, image_array.shape)\n    image_add = np.clip(image_add, 0, 255).astype(np.uint8)\n    return image_add\n\ndef flip_image(image_array):\n    return cv2.flip(image_array, 1)\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef generate_flip_grid(w, h, device):\n    # used to flip attention maps\n    x_ = torch.arange(w).view(1, -1).expand(h, -1)\n    y_ = torch.arange(h).view(-1, 1).expand(-1, w)\n    grid = torch.stack([x_, y_], dim=0).float().to(device)\n    grid = grid.unsqueeze(0).expand(1, -1, -1, -1)\n    grid[:, 0, :, :] = 2 * grid[:, 0, :, :] / (w - 1) - 1\n    grid[:, 1, :, :] = 2 * grid[:, 1, :, :] / (h - 1) - 1\n    grid[:, 0, :, :] = -grid[:, 0, :, :]\n    return grid\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.302045Z","iopub.execute_input":"2024-06-08T13:23:11.302572Z","iopub.status.idle":"2024-06-08T13:23:11.312351Z","shell.execute_reply.started":"2024-06-08T13:23:11.302532Z","shell.execute_reply":"2024-06-08T13:23:11.311474Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class RafDataset(data.Dataset):\n    def __init__(self, args, phase, basic_aug=True, transform=None):\n        self.raf_path = args.raf_path\n        self.phase = phase\n        self.basic_aug = basic_aug\n        self.transform = transform\n        df = pd.read_csv(os.path.join(self.raf_path, 'EmoLabel', args.label_path), sep=' ', header=None)\n\n        name_c = 0\n        label_c = 1\n        if phase == 'train':\n            dataset = df[df[name_c].str.startswith('train')]\n        else:\n            df = pd.read_csv(os.path.join(self.raf_path, 'EmoLabel/list_patition_label.txt'), sep=' ', header=None)\n            dataset = df[df[name_c].str.startswith('test')]\n\n        self.label = dataset.iloc[:, label_c].values - 1\n        images_names = dataset.iloc[:, name_c].values\n        self.aug_func = [flip_image, add_g]\n        self.file_paths = []\n        self.clean = (args.label_path == 'list_patition_label.txt')\n\n        for f in images_names:\n            f = f.split(\".\")[0]\n            f += '_aligned.jpg'\n            file_name = os.path.join(self.raf_path, 'DATASET', f)\n            self.file_paths.append(file_name)\n\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        label = self.label[idx]\n        image = cv2.imread(self.file_paths[idx])\n\n        image = image[:, :, ::-1]\n\n\n        if not self.clean:\n            image1 = image\n            image1 = self.aug_func[0](image)\n            image1 = self.transform(image1)\n\n        if self.phase == 'train':\n            if self.basic_aug and random.uniform(0, 1) > 0.5:\n                image = self.aug_func[1](image)\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        if self.clean:\n            image1 = transforms.RandomHorizontalFlip(p=1)(image)\n\n        return image, label, idx, image1","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.313580Z","iopub.execute_input":"2024-06-08T13:23:11.313912Z","iopub.status.idle":"2024-06-08T13:23:11.327980Z","shell.execute_reply.started":"2024-06-08T13:23:11.313884Z","shell.execute_reply":"2024-06-08T13:23:11.327229Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"from torch.autograd import Variable\n\ndef ACLoss(att_map1, att_map2, grid_l, output):\n    flip_grid_large = grid_l.expand(output.size(0), -1, -1, -1)\n    flip_grid_large = Variable(flip_grid_large, requires_grad = False)\n    flip_grid_large = flip_grid_large.permute(0, 2, 3, 1)\n    att_map2_flip = F.grid_sample(att_map2, flip_grid_large, mode = 'bilinear', padding_mode = 'border', align_corners=True)\n    flip_loss_l = F.mse_loss(att_map1, att_map2_flip)\n    return flip_loss_l\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.328954Z","iopub.execute_input":"2024-06-08T13:23:11.329229Z","iopub.status.idle":"2024-06-08T13:23:11.339490Z","shell.execute_reply.started":"2024-06-08T13:23:11.329208Z","shell.execute_reply":"2024-06-08T13:23:11.338717Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\nmodel = Model()\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)\nsetup_seed(0)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.340425Z","iopub.execute_input":"2024-06-08T13:23:11.340720Z","iopub.status.idle":"2024-06-08T13:23:11.475017Z","shell.execute_reply.started":"2024-06-08T13:23:11.340690Z","shell.execute_reply":"2024-06-08T13:23:11.474025Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Train and Test\n","metadata":{}},{"cell_type":"code","source":"\n\n\ndef train(args, model, train_loader, optimizer, scheduler, device):\n    running_loss = 0.0\n    iter_cnt = 0\n    correct_sum = 0\n\n    model.to(device)\n    model.train()\n\n    total_loss = []\n    for batch_i, (imgs1, labels, indexes, imgs2) in enumerate(train_loader):\n        imgs1 = imgs1.to(device)\n        imgs2 = imgs2.to(device)\n        labels = labels.to(device)\n\n\n        criterion = nn.CrossEntropyLoss(reduction='none')\n\n\n\n        output, hm1 = model(imgs1)\n        output_flip, hm2 = model(imgs2)\n\n        grid_l = generate_flip_grid(args.w, args.h, device)\n\n\n        loss1 = nn.CrossEntropyLoss()(output, labels)\n        flip_loss_l = ACLoss(hm1, hm2, grid_l, output)\n\n\n        loss = loss1 + args.lam * flip_loss_l\n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n        iter_cnt += 1\n        _, predicts = torch.max(output, 1)\n        correct_num = torch.eq(predicts, labels).sum()\n        correct_sum += correct_num\n        running_loss += loss\n\n    scheduler.step()\n    running_loss = running_loss / iter_cnt\n    acc = correct_sum.float() / float(train_loader.dataset.__len__())\n    return acc, running_loss\n\n\n\ndef test(model, test_loader, device):\n    with torch.no_grad():\n        model.eval()\n\n        running_loss = 0.0\n        iter_cnt = 0\n        correct_sum = 0\n        data_num = 0\n\n\n        for batch_i, (imgs1, labels, indexes, imgs2) in enumerate(test_loader):\n            imgs1 = imgs1.to(device)\n            labels = labels.to(device)\n\n\n            outputs, _ = model(imgs1)\n\n\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n\n            iter_cnt += 1\n            _, predicts = torch.max(outputs, 1)\n\n            correct_num = torch.eq(predicts, labels).sum()\n            correct_sum += correct_num\n\n            running_loss += loss\n            data_num += outputs.size(0)\n\n        running_loss = running_loss / iter_cnt\n        test_acc = correct_sum.float() / float(data_num)\n    return test_acc, running_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:23:11.477642Z","iopub.execute_input":"2024-06-08T13:23:11.477900Z","iopub.status.idle":"2024-06-08T13:23:11.490894Z","shell.execute_reply.started":"2024-06-08T13:23:11.477878Z","shell.execute_reply":"2024-06-08T13:23:11.489988Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"train_loss = []\ntest_loss = []\ntrain_acc = []\ntest_acc = [] \nmin_loss = 9999999999999\n\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(scale=(0.02, 0.25)) ])\n\neval_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])])\n\n\n\ntrain_dataset = RafDataset(args, phase='train', transform=train_transforms)\ntest_dataset = RafDataset(args, phase='test', transform=eval_transforms)\n\n\ntrain_loader = DataLoader(train_dataset,\n                         batch_size=args.batch_size,\n                         shuffle=True,\n                         num_workers=args.workers,\n                         pin_memory=True)\n\ntest_loader = DataLoader(test_dataset, \n                         batch_size=args.batch_size,\n                         shuffle=False,\n                         num_workers=args.workers,\n                         pin_memory=True)   \n\noptimizer = torch.optim.Adam(model.parameters() , lr=0.0001, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\nwandb.login(key = '80fb920b9b9c938e38c91805ea14911702e078be')\nwandb.init(project = 'Facial Emotion Recognition')\n\n   \n    \nfor i in range(1, args.epochs + 1):\n    train_acc_epoch, train_loss_epoch = train(args, model, train_loader, optimizer, scheduler, device)\n    train_acc.append(train_acc_epoch)\n    train_loss.append(train_loss_epoch)\n\n    test_acc_epoch, test_loss_epoch = test(model, test_loader, device)\n    if(test_loss_epoch < min_loss):\n        min_loss = test_loss_epoch\n        torch.save(model.state_dict(), 'mobilenet_v2.pth')\n    test_acc.append(test_acc_epoch)\n    test_loss.append(test_loss_epoch)\n    print(str(i)+'_'+str(train_acc_epoch) +str(test_acc_epoch)+'\\n')\n\n    wandb.log({\"Train loss\": train_loss_epoch, \"Test loss\": test_loss_epoch,\n              \"Train_acc\": train_acc_epoch, \"Test acc\": test_acc_epoch})\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:32:41.127288Z","iopub.execute_input":"2024-06-08T13:32:41.128237Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:wxzvt8lr) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">helpful-glade-15</strong> at: <a href='https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition/runs/wxzvt8lr' target=\"_blank\">https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition/runs/wxzvt8lr</a><br/> View project at: <a href='https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition' target=\"_blank\">https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240608_132859-wxzvt8lr/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:wxzvt8lr). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240608_133241-1o9j5a12</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition/runs/1o9j5a12' target=\"_blank\">absurd-wave-16</a></strong> to <a href='https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition' target=\"_blank\">https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition/runs/1o9j5a12' target=\"_blank\">https://wandb.ai/hung123ka5/Facial%20Emotion%20Recognition/runs/1o9j5a12</a>"},"metadata":{}},{"name":"stdout","text":"1_tensor(0.7246, device='cuda:0')tensor(0.7823, device='cuda:0')\n\n2_tensor(0.7670, device='cuda:0')tensor(0.7947, device='cuda:0')\n\n3_tensor(0.7910, device='cuda:0')tensor(0.8129, device='cuda:0')\n\n4_tensor(0.8087, device='cuda:0')tensor(0.8142, device='cuda:0')\n\n5_tensor(0.8239, device='cuda:0')tensor(0.8295, device='cuda:0')\n\n6_tensor(0.8308, device='cuda:0')tensor(0.8409, device='cuda:0')\n\n7_tensor(0.8449, device='cuda:0')tensor(0.8449, device='cuda:0')\n\n8_tensor(0.8550, device='cuda:0')tensor(0.8367, device='cuda:0')\n\n9_tensor(0.8577, device='cuda:0')tensor(0.8439, device='cuda:0')\n\n10_tensor(0.8670, device='cuda:0')tensor(0.8475, device='cuda:0')\n\n11_tensor(0.8743, device='cuda:0')tensor(0.8537, device='cuda:0')\n\n12_tensor(0.8778, device='cuda:0')tensor(0.8553, device='cuda:0')\n\n13_tensor(0.8869, device='cuda:0')tensor(0.8582, device='cuda:0')\n\n14_tensor(0.8856, device='cuda:0')tensor(0.8576, device='cuda:0')\n\n15_tensor(0.8968, device='cuda:0')tensor(0.8569, device='cuda:0')\n\n16_tensor(0.8917, device='cuda:0')tensor(0.8585, device='cuda:0')\n\n17_tensor(0.9008, device='cuda:0')tensor(0.8615, device='cuda:0')\n\n18_tensor(0.8987, device='cuda:0')tensor(0.8592, device='cuda:0')\n\n19_tensor(0.9042, device='cuda:0')tensor(0.8625, device='cuda:0')\n\n20_tensor(0.9112, device='cuda:0')tensor(0.8634, device='cuda:0')\n\n21_tensor(0.9129, device='cuda:0')tensor(0.8582, device='cuda:0')\n\n22_tensor(0.9085, device='cuda:0')tensor(0.8572, device='cuda:0')\n\n23_tensor(0.9107, device='cuda:0')tensor(0.8589, device='cuda:0')\n\n24_tensor(0.9140, device='cuda:0')tensor(0.8592, device='cuda:0')\n\n25_tensor(0.9146, device='cuda:0')tensor(0.8602, device='cuda:0')\n\n26_tensor(0.9161, device='cuda:0')tensor(0.8611, device='cuda:0')\n\n27_tensor(0.9138, device='cuda:0')tensor(0.8615, device='cuda:0')\n\n28_tensor(0.9163, device='cuda:0')tensor(0.8625, device='cuda:0')\n\n29_tensor(0.9192, device='cuda:0')tensor(0.8608, device='cuda:0')\n\n30_tensor(0.9138, device='cuda:0')tensor(0.8634, device='cuda:0')\n\n31_tensor(0.9179, device='cuda:0')tensor(0.8605, device='cuda:0')\n\n32_tensor(0.9159, device='cuda:0')tensor(0.8602, device='cuda:0')\n\n33_tensor(0.9214, device='cuda:0')tensor(0.8625, device='cuda:0')\n\n34_tensor(0.9185, device='cuda:0')tensor(0.8628, device='cuda:0')\n\n35_tensor(0.9226, device='cuda:0')tensor(0.8644, device='cuda:0')\n\n36_tensor(0.9183, device='cuda:0')tensor(0.8638, device='cuda:0')\n\n37_tensor(0.9219, device='cuda:0')tensor(0.8608, device='cuda:0')\n\n38_tensor(0.9249, device='cuda:0')tensor(0.8625, device='cuda:0')\n\n39_tensor(0.9234, device='cuda:0')tensor(0.8611, device='cuda:0')\n\n40_tensor(0.9222, device='cuda:0')tensor(0.8602, device='cuda:0')\n\n41_tensor(0.9224, device='cuda:0')tensor(0.8641, device='cuda:0')\n\n42_tensor(0.9258, device='cuda:0')tensor(0.8595, device='cuda:0')\n\n43_tensor(0.9238, device='cuda:0')tensor(0.8621, device='cuda:0')\n\n44_tensor(0.9245, device='cuda:0')tensor(0.8615, device='cuda:0')\n\n45_tensor(0.9232, device='cuda:0')tensor(0.8634, device='cuda:0')\n\n46_tensor(0.9233, device='cuda:0')tensor(0.8618, device='cuda:0')\n\n47_tensor(0.9236, device='cuda:0')tensor(0.8621, device='cuda:0')\n\n48_tensor(0.9268, device='cuda:0')tensor(0.8572, device='cuda:0')\n\n49_tensor(0.9231, device='cuda:0')tensor(0.8631, device='cuda:0')\n\n50_tensor(0.9230, device='cuda:0')tensor(0.8634, device='cuda:0')\n\n51_tensor(0.9258, device='cuda:0')tensor(0.8621, device='cuda:0')\n\n52_tensor(0.9226, device='cuda:0')tensor(0.8638, device='cuda:0')\n\n53_tensor(0.9275, device='cuda:0')tensor(0.8638, device='cuda:0')\n\n54_tensor(0.9250, device='cuda:0')tensor(0.8657, device='cuda:0')\n\n55_tensor(0.9212, device='cuda:0')tensor(0.8647, device='cuda:0')\n\n56_tensor(0.9252, device='cuda:0')tensor(0.8644, device='cuda:0')\n\n57_tensor(0.9256, device='cuda:0')tensor(0.8638, device='cuda:0')\n\n58_tensor(0.9246, device='cuda:0')tensor(0.8589, device='cuda:0')\n\n59_tensor(0.9245, device='cuda:0')tensor(0.8634, device='cuda:0')\n\n60_tensor(0.9217, device='cuda:0')tensor(0.8625, device='cuda:0')\n\n61_tensor(0.9232, device='cuda:0')tensor(0.8605, device='cuda:0')\n\n62_tensor(0.9275, device='cuda:0')tensor(0.8644, device='cuda:0')\n\n63_tensor(0.9225, device='cuda:0')tensor(0.8628, device='cuda:0')\n\n64_tensor(0.9258, device='cuda:0')tensor(0.8628, device='cuda:0')\n\n65_tensor(0.9236, device='cuda:0')tensor(0.8634, device='cuda:0')\n\n66_tensor(0.9268, device='cuda:0')tensor(0.8592, device='cuda:0')\n\n67_tensor(0.9255, device='cuda:0')tensor(0.8605, device='cuda:0')\n\n68_tensor(0.9253, device='cuda:0')tensor(0.8647, device='cuda:0')\n\n69_tensor(0.9249, device='cuda:0')tensor(0.8625, device='cuda:0')\n\n70_tensor(0.9287, device='cuda:0')tensor(0.8618, device='cuda:0')\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:25:12.078949Z","iopub.status.idle":"2024-06-08T13:25:12.079509Z","shell.execute_reply.started":"2024-06-08T13:25:12.079231Z","shell.execute_reply":"2024-06-08T13:25:12.079255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:25:12.081769Z","iopub.status.idle":"2024-06-08T13:25:12.082675Z","shell.execute_reply.started":"2024-06-08T13:25:12.082370Z","shell.execute_reply":"2024-06-08T13:25:12.082410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T13:25:12.084224Z","iopub.status.idle":"2024-06-08T13:25:12.085371Z","shell.execute_reply.started":"2024-06-08T13:25:12.085110Z","shell.execute_reply":"2024-06-08T13:25:12.085131Z"},"trusted":true},"execution_count":null,"outputs":[]}]}